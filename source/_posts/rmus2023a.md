---
title: Rmus et al. (2023a)
subtitle: Choice Type Impacts Human Reinforcement Learning
date: 2023/02/01
authors:
- Rmus, Milena
- Zou, Amy
- Collins, Anne G E
journal: J. Cogn. Neurosci.
paper_url: https://doi.org/10.1162/jocn_a_01947
data_url: https://osf.io/vehtk/?view_only=b05b15c7301f4214bb12080ad690935b
tags:
- multi-arm bandit
- working memory
sample_size: 157
---

In reinforcement learning (RL) experiments, participants learn to make rewarding choices in response to different stimuli; RL models use outcomes to estimate stimulusâ€“response values that change incrementally. RL models consider any response type indiscriminately, ranging from more concretely defined motor choices (pressing a key with the index finger), to more general choices that can be executed in a number of ways (selecting dinner at the restaurant). However, does the learning process vary as a function of the choice type? In Experiment 1, we show that it does: Participants were slower and less accurate in learning correct choices of a general format compared with learning more concrete motor actions. Using computational modeling, we show that two mechanisms contribute to this. First, there was evidence of irrelevant credit assignment: The values of motor actions interfered with the values of other choice dimensions, resulting in more incorrect choices when the correct response was not defined by a single motor action; second, information integration for relevant general choices was slower. In Experiment 2, we replicated and further extended the findings from Experiment 1 by showing that slowed learning was attributable to weaker working memory use, rather than slowed RL. In both experiments, we ruled out the explanation that the difference in performance between two condition types was driven by difficulty/different levels of complexity. We conclude that defining a more abstract choice space used by multiple learning systems for credit assignment recruits executive resources, limiting how much such processes then contribute to fast learning.
